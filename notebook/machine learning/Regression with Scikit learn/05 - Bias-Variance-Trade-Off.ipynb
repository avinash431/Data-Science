{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and the bias-variance trade-off\n",
    "\n",
    "**Overfitting** is a constant danger with machine learning models. Overfit models fit the training data well. However, an overfit model will not **generalize**. A model that generalizes is a model which exhibits good performance on data cases beyond the ones used in training. Models that generalize will be useful in production. \n",
    "\n",
    "As a general rule, an overfit model has learned the training data too well. The overfitting likely involved learning noise present in the training data. The noise in the data is random and uninformative. When a new data case is presented to such a model it may produce unexpected results since the random noise will be different. \n",
    "\n",
    "So, what is one to do to prevent overfitting of machine learning models? The most widely used set of tools for preventing overfitting are known as **regularization methods**. Regularization methods take a number of forms, but all have the same goal, to prevent overfitting of machine learning models. \n",
    "\n",
    "Regularization is not free however. While regularization reduces the **variance** in the model results, it introduces **bias**. Whereas, an overfit model exhibits low bias the variance is high. The high variance leads to unpredictable results when the model is exposed to new data cases. On the other hand, the stronger the regularization of a model the lower the variance, but the greater the bias. This all means that when applying regularization you will need to contend with the **bias-variance trade-off**. \n",
    "\n",
    "To better understand the bias variance trade-off consider the following examples of extreme model cases:\n",
    "\n",
    "- If the prediction for all cases is just the mean (or median), variance is minimized. The estimate for all cases is the same, so the bias of the estimates is zero. However, there is likely considerable variance in these estimates. \n",
    "- On the other hand, consider what happens when the data are fit with a kNN model with k=1. The training data will fit this model perfectly, since there is one model coefficient per training data point. The variance will be low. However, the model will have considerable bias when applied to test data. \n",
    "\n",
    "In either case, these extreme models will not generalize well and will exhibit large errors on any independent test data. Any practical model must come to terms with the trade-off between bias and variance to make accurate predictions. \n",
    "\n",
    "To better understand this trade-off you can consider the example of the mean square error, which can be decomposed into its components. The mean square error can be written as:\n",
    "\n",
    "$$\\Delta x = E \\Big[ \\big(Y - \\hat{f}(X) \\big)^2 \\Big] = \\frac{1}{N} \\sum_{i=1}^N \\big(y_i - \\hat{f}(x_i) \\big)^2 $$\n",
    "\n",
    "Where,\n",
    "$Y = $ the label vector.  \n",
    "$X = $ the feature matrix.   \n",
    "$\\hat{f}(x) = $ the trained model.   \n",
    "\n",
    "Expanding the representation of the mean square error:\n",
    "\n",
    "$$\\Delta x = \\big( E[ \\hat{f}(X)] - \\hat{f}(X) \\big)^2 + E \\big[ ( \\hat{f}(X) - E[ \\hat{f}(X)])^2 \\big] + \\sigma^2\\\\\n",
    "\\Delta x = Bias^2 + Variance + Irreducible\\ Error$$\n",
    "\n",
    "Study this relationship. Notice that as regularization reduces variance, bias increases. The irreducible error will remain unchanged. Regularization parameters are chosen to minimize $\\Delta x$. In many cases, this will prove challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l2 regularization\n",
    "\n",
    "Now, you will apply **l2 regularization** to constrains the model parameters. Constraining the model parameters prevent overfitting of the model. This method is also known as **Ridge Regression**. \n",
    "\n",
    "But, how does this work? l2 regularization applies a **penalty** proportional to the **l2** or **Eucldian norm** of the model weights to the loss function. For linear regression using squared error as the metric, the total **loss function** is the sum of the squared error and the regularization term. The total loss function can then be written as follows:  \n",
    "\n",
    "$$J(\\beta) = ||A \\beta + b||^2 + \\lambda ||\\beta||^2$$\n",
    "\n",
    "Where the penalty term on the model coefficients, $\\beta_i$, is written:\n",
    "\n",
    "$$\\lambda || \\beta||^2 = \\lambda \\big(\\beta_1^2 + \\beta_2^2 + \\ldots + \\beta_n^2 \\big)^{\\frac{1}{2}} = \\lambda \\Big( \\sum_{i=1}^n \\beta_i^2 \\Big)^{\\frac{1}{2}}$$\n",
    "\n",
    "We call $||\\beta||^2$ the **l2 norm** of the coefficients, since we raise the weights of each coefficient to the power of 2, sum the squares and then raise the sum to the power of $\\frac{1}{2}$. \n",
    "\n",
    "You can think of this penalty as constraining the 12 or Euclidean norm of the model weight vector. The value of $\\lambda$ determines how much the norm of the coefficient vector constrains the solution. You can see a geometric interpretation of the l2 penalty constraint in the figure below.  \n",
    "\n",
    "<img src=\"img/L2.jpg\" alt=\"Drawing\" style=\"width:750px; height:400px\"/>\n",
    "<center> **Geometric view of l2 regularization**\n",
    "\n",
    "Notice that for a constant value of the l2 norm, the values of the model parameters $B_1$ and $B_2$ are related. The Euclidian or l2 norm of the coefficients is shown as the dotted circle. The constant value of the l2 norm is a constant value of the penalty. Along this circle the coefficients change in relation to each other to maintain a constant l2 norm. For example, if $B_1$ is maximized then $B_2 \\sim 0$, or vice versa. It is important to note that l2 regularization is a **soft constraint**. Coefficients are driven close to, but likely not exactly to, zero.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l1 regularizaton\n",
    "\n",
    "Regularization can be performed using norms other than l2. The **l1 regularizaton** or **Lasso**  method limits the sum of the absolute values of the model coefficients. The l1 norm is sometime know as the **Manhattan norm**, since distance are measured as if you were traveling on a rectangular grid of streets. This is in contrast to the l2 norm that measures distance 'as the crow flies'. \n",
    "\n",
    "We can compute the l1 norm of the model coefficients as follows:\n",
    "\n",
    "$$||\\beta||^1 = \\big( |\\beta_1| + |\\beta_2| + \\ldots + |\\beta_n| \\big) = \\Big( \\sum_{i=1}^n |\\beta_i| \\Big)^1$$\n",
    "\n",
    "where $|\\beta_i|$ is the absolute value of $\\beta_i$. \n",
    "\n",
    "Notice that to compute the l1 norm, we raise the sum of the absolute values to the first power.\n",
    "\n",
    "As with l2 regularization, for l1 regularization, a penalty term is multiplied by the l1 norm of the model coefficients. A penalty multiplier, $\\lambda$, determines how much the norm of the coefficient vector constrains values of the weights. The complete loss function is the sum of the squared errors plus the penalty term which becomes: \n",
    "\n",
    "$$J(\\beta) = ||A \\beta + b||^2 + \\lambda ||\\beta||^1$$\n",
    "\n",
    "You can see a geometric interpretation of the l1 norm penalty in the figure below.  \n",
    "\n",
    "<img src=\"img/L1.jpg\" alt=\"Drawing\" style=\"width:700px; height:400px\"/>\n",
    "<center> **Geometric view of L1 regularization**\n",
    "\n",
    "The l1 norm is constrained by the sum of the absolute values of the coefficients. This fact means that values of one parameter highly constrain another parameter. The dotted line in the figure above looks as though someone has pulled a rope or lasso around pegs on the axes. This behavior leads the name lasso for l1 regularization.  \n",
    "\n",
    "Notice that in the figure above that if $B_1 = 0$ then $B_2$ has a value at the limit, or vice versa. In other words, using a l1 norm constraint forces some weight values to zero to allow other coefficients to take non-zero values. Thus, you can think of the l1 norm constraint **knocking out** some weights free the model altogether. In contrast to l2 regularization, l1 regularization does drive some coefficients to exactly zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

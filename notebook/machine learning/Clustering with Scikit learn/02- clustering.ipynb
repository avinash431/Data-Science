{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Clustering\n",
    "* Partitioning methods\n",
    "  - Partitions n data into k partitions\n",
    "  - Initially, random partitions are created & gradually data is moved across different partitions.\n",
    "  - It uses distance between points to optimize clusters.\n",
    "  - KMeans & Meanshift are examples of Partitioning methods\n",
    "* Hierarchical methods\n",
    "  - These methods does hierarchical decomposition of datasets.\n",
    "  - One approach is, assume each data as cluster & merge to create a bigger cluster\n",
    "  - Another approach is start with one cluster & continue splitting\n",
    "* Density-based methods\n",
    "  - All above techniques are distance based & such methods can find only spherical clusters and not suited for clusters of other shapes.\n",
    "  - Continue growing the cluster untill the density exceeds certain threashold.\n",
    "  \n",
    "# Clustering as an Optimization Problem\n",
    "* Maximize inter-cluster distances\n",
    "* Minimize intra-cluster distances\n",
    "\n",
    "## K-means clustering\n",
    "\n",
    "K-means clustering separates a dataset into K clusters of equal variance. The number of clusters, K, is user defined. The basic algorithm has the following steps:\n",
    "1. A set of K centroids are randomly chosen. \n",
    "2. Clusters are formed by minimizing variance within each cluster. This metric is also know as the **within cluster sum of squares** (see further discussion in the section on evaluating clusters). This step partitions the data into clusters with minimum squared distance to the centroid of the cluster. \n",
    "3. The centroids are moved to mean of each cluster. The means of each cluster is computed and the centroid is moved to the mean. \n",
    "4. Steps 2 and 3 are repeated until a stopping criteria is met. Typically, the algorithm terminates when the within cluster variance decreases only minimally. \n",
    "5. The above steps are repeated starting with a random start of step 1. The best set of clusters by within cluster variance and between cluster separation are retained.  \n",
    "\n",
    "Since K-means clustering relies only on basic linear algebra operations, the method is massively scalable. Out-of-core K-means clustering algorithms are widely used. However, this method assumes equal variance of the clusters, a fairly restrictive assumption. In practice, this criteria is almost never true, and yet K-means clustering still produces useful results. \n",
    "\n",
    "* Minimizing creteria : within-cluster-sum-of-squares.\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/kmeans2.png?raw=true\">\n",
    "\n",
    "* The centroids are chosen in such a way that it minimizes within cluster sum of squares.\n",
    "\n",
    "* The k-means algorithm divides a set of $N$ samples $X$ into $K$ disjoint clusters $C$, each described by the mean  of the samples in the cluster. $\\mu$\n",
    "\n",
    "<img src=\"https://cssanalytics.files.wordpress.com/2013/11/cluster-image.png\" width=\"300px\">\n",
    "\n",
    "##### KMeans Algorithm\n",
    "1. Initialize k centroids.\n",
    "2. Assign each data to the nearest centroid, these step will create clusters.\n",
    "3. Recalculate centroid - which is mean of all data belonging to same cluster.\n",
    "4. Repeat steps 2 & 3, till there is no data to reassign a different centroid.\n",
    "\n",
    "Animation to explain algo - http://tech.nitoyon.com/en/blog/2013/11/07/k-means/\n",
    "\n",
    "Several different distance metrics are used to compute linkage functions:\n",
    "- **Euclidian** or **l2** distance is the most widely used. This metric is only choice for the Ward linkage method. \n",
    "- **Manhattan** or **l1** distance is robust to outliers and has other interesting properties. \n",
    "- **Cosine** similarity, is the dot product between the location vectors divided by the magnitudes of the vectors. Notice that this metric is a measure of similarity, whereas the other two metrics are measures of difference. Similarity can be quite useful when working with data such as images or text documents. \n",
    "\n",
    "Distance or Similarity Function\n",
    "* Data belonging to same cluster are similar & data belonging to different cluster are different. \n",
    "* We need mechanisms to measure similarity & differences between data. \n",
    "* This can be achieved using any of the below techniques.\n",
    "\n",
    " - Minkowiski breed of distance calculation: \n",
    " \n",
    " <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/4060cc840aeab9e41b5e47356088889e2e7a6f0f\">\n",
    " \n",
    " - Manhatten (p=1), Euclidian (p=2)\n",
    " \n",
    " - Cosine: Suited for text data\n",
    " \n",
    " <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1d94e5903f7936d3c131e040ef2c51b473dd071d\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances,cosine_distances,manhattan_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances([[3,4],[4,5]],[[0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0, 1], [1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances(X, [[0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances(X,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_distances(X,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_blobs(n_features=2, n_samples=1000, cluster_std=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1],s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1],s=10, c=kmeans.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use elbow method to calculate k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interias  = []\n",
    "\n",
    "for i in range(1,10):\n",
    "    k = KMeans(n_clusters=i)\n",
    "    k.fit(X)\n",
    "    interias.append(k.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(interias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=.05)\n",
    "plt.scatter(X[:,0], X[:,1],s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1],s=10, c=kmeans.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of KMeans\n",
    "* Assumes that clusters are convex & behaves poorly for elongated clusters.\n",
    "* Probability for participation of data to multiple clusters.\n",
    "* KMeans tries to find local minima & this depends on init value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchial Clustering\n",
    "* A method of clustering where you combine similar clusters to create a cluster or split a cluster into smaller clusters such they now they become better.\n",
    "* Two types of hierarchaial Clustering\n",
    "  - Agglomerative method, a botton-up approach.\n",
    "  - Divisive method, a top-down approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative method\n",
    "* Start with assigning one cluster to each data. \n",
    "* Combine clusters which have higher similarity.\n",
    "* Differences between methods arise due to different ways of defining distance (or similarity) between clusters. The following sections describe several agglomerative techniques in detail.\n",
    "  - Single Linkage Clustering\n",
    "  - Complete linkage clustering\n",
    "  - Average linkage clustering\n",
    "  - Average group linkage\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, linkage, dendrogram\n",
    "np.set_printoptions(precision=4,suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = linkage(X,\"ward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "plt.title(\"Hiearcharial clustering\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "dendrogram(distance,leaf_rotation=90,leaf_font_size=9,orientation=\"left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "plt.title(\"Hiearcharial clustering\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "dendrogram(distance,leaf_rotation=90,leaf_font_size=9,truncate_mode=\"lastp\",show_contracted=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agc = AgglomerativeClustering(linkage='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1],s=10,c=agc.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Based Clustering - DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1],s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1],s=10,c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Performance of Clusters\n",
    "* Two forms of evaluation \n",
    "* supervised, which uses a ground truth class values for each sample.\n",
    "  - completeness_score\n",
    "  - homogeneity_score\n",
    "* unsupervised, which measures the quality of model itself\n",
    "  - silhoutte_score\n",
    "  - calinski_harabaz_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### completeness_score\n",
    "- A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "- Accuracy is 1.0 if data belonging to same class belongs to same cluster, even if multiple classes belongs to same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import completeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_score(labels_true=[10,10,11,11],labels_pred=[1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_score(labels_true=[11,22,22,11],labels_pred=[1,0,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The accuracy is .3 because class 1 - [11,22,11], class 2 - [22] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### homogeneity_score\n",
    "- A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import homogeneity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_score([0, 0, 0, 0], [1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Same class data is broken into two clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### silhoutte_score\n",
    "* The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample.\n",
    "* The Silhouette Coefficient for a sample is (b - a) / max(a, b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, Y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = [2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_cluster in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_cluster)\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.predict(X)\n",
    "    print (n_cluster, silhouette_score(X,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimal number of clusters seems to be 2\n",
    "\n",
    "#### calinski_harabaz_score\n",
    "* The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabaz_score\n",
    "\n",
    "for n_cluster in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_cluster)\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.predict(X)\n",
    "    print (n_cluster, calinski_harabaz_score(X,labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
